

原生PG的缺陷
首先我们需要考虑的便是队列的长度。就如我们在《深度强化学习·当我们换一种游戏》中提及的frozen-lake与cart-pole游戏的区别，在完成一个episode的过程中，序列的长度是不可控的，这种不可控会导致我们必须在进行完一个episode之后才能计算Q值。
其次我们发现原始的PG算法中并没有增加贪婪策略，每个episode产生的序列的动作决策都是由神经网络输出概率最大的动作，这会导致对环境的搜索不够全面
Q值没有一个固定的范围大小。这也就是说Q的量级（scale）不受控制。以cart-pole为例子，第一步的奖励为1，第一百步的累计奖励就是100，这直接导致了Q值的方差过大。导致训练过程不稳定。
对于正常决策来说，如果某state1下不同动作的Q值有以下情况：Q1=120，Q2=60；而另一个state2下有以下情况：Q1=30，Q2=-30。如果两个状态下网络输出的动作概率也是相同的，那么网络优化的方向应该相同：增加Q1，减小Q2。但是对于交叉熵来说，两个state优化的方向是不同的：对于state1，交叉熵给出的结论是大幅增加动作1的概率，小幅增加动作2的概率；对于state2，交叉熵的结论是应该增加动作1的概率，减少动作2的概率。我们希望在Q值分布类似的情况下，网络优化的方向应该相同。



1. 如何避免自杀倾向
2. 单步最优  如何与结果最优优化方向一致


模型的拟合 nowStatus + 代表了模型对算法中算子  + Action  = newStatus  的预测是准确的

但模型在下一步action的选择中 

游戏规则从 放一块后更新item 变成放n块后更新item

训练模式和运行模式区分

状态的设计 有点问题
batch的设计

使用策略梯度的方式去更新网络